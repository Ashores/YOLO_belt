# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import torch
import torch.nn as nn
import torch.nn.functional as F

from .metrics import bbox_iou
from .tal import bbox2dist
import pdb

class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2, num_classes=80, size_average=False):
        """
        focal_lossæŸå¤±å‡½æ•°, -Î±(1-yi)**Î³ *ce_loss(xi,yi)
        æ­¥éª¤è¯¦ç»†çš„å®ç°äº† focal_lossæŸå¤±å‡½æ•°.
        :param alpha:   é˜¿å°”æ³•Î±,ç±»åˆ«æƒé‡.      å½“Î±æ˜¯åˆ—è¡¨æ—¶,ä¸ºå„ç±»åˆ«æƒé‡,å½“Î±ä¸ºå¸¸æ•°æ—¶,ç±»åˆ«æƒé‡ä¸º[Î±, 1-Î±, 1-Î±, ....],å¸¸ç”¨äº ç›®æ ‡æ£€æµ‹ç®—æ³•ä¸­æŠ‘åˆ¶èƒŒæ™¯ç±» , retainnetä¸­è®¾ç½®ä¸º0.25
        :param gamma:   ä¼½é©¬Î³,éš¾æ˜“æ ·æœ¬è°ƒèŠ‚å‚æ•°. retainnetä¸­è®¾ç½®ä¸º2
        :param num_classes:     ç±»åˆ«æ•°é‡
        :param size_average:    æŸå¤±è®¡ç®—æ–¹å¼,é»˜è®¤å–å‡å€¼
        """
        super(FocalLoss, self).__init__()
        self.size_average = size_average
        if alpha is None:
            self.alpha = torch.ones(num_classes)
        elif isinstance(alpha, list):
            assert len(alpha) == num_classes  # Î±å¯ä»¥ä»¥listæ–¹å¼è¾“å…¥,size:[num_classes] ç”¨äºå¯¹ä¸åŒç±»åˆ«ç²¾ç»†åœ°èµ‹äºˆæƒé‡
            self.alpha = torch.Tensor(alpha)
        else:
            assert alpha < 1  # å¦‚æœÎ±ä¸ºä¸€ä¸ªå¸¸æ•°,åˆ™é™ä½ç¬¬ä¸€ç±»çš„å½±å“,åœ¨ç›®æ ‡æ£€æµ‹ä¸­ç¬¬ä¸€ç±»ä¸ºèƒŒæ™¯ç±»
            self.alpha = torch.zeros(num_classes)
            self.alpha[0] += alpha
            self.alpha[1:] += (1 - alpha)  # Î± æœ€ç»ˆä¸º [ Î±, 1-Î±, 1-Î±, 1-Î±, 1-Î±, ...] size:[num_classes]

        self.gamma = gamma

        # print('Focal Loss:')
        # print('    Alpha = {}'.format(self.alpha))
        # print('    Gamma = {}'.format(self.gamma))

    def forward(self, preds, labels):
        preds = preds.view(-1, preds.size(-1))
        self.alpha = self.alpha.to(preds.device)
        preds_logsoft = F.log_softmax(preds, dim=1)  # log_softmax
        preds_softmax = torch.exp(preds_logsoft)  # softmax
        labels = labels.to(torch.int64)
        preds_softmax = preds_softmax.gather(1, labels.view(-1, 1))  # è¿™éƒ¨åˆ†å®ç°nll_loss ( crossempty = lo  g_softmax + nll )
        preds_logsoft = preds_logsoft.gather(1, labels.view(-1, 1))
        alpha = self.alpha.gather(0, labels.view(-1))
        # print(alpha.is_cuda, labels.is_cuda)
        loss = -torch.mul(torch.pow((1 - preds_softmax), self.gamma),
                          preds_logsoft)  # torch.pow((1-preds_softmax), self.gamma) ä¸ºfocal lossä¸­ (1-pt)**Î³

        loss = torch.mul(alpha, loss.t())
        if self.size_average:
            loss = loss.mean()
        else:
            loss = loss.sum()
        return loss

class VarifocalLoss(nn.Module):
    """Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367."""

    def __init__(self):
        """Initialize the VarifocalLoss class."""
        super().__init__()

    def forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):
        """Computes varfocal loss."""
        weight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label
        with torch.cuda.amp.autocast(enabled=False):
            loss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *
                    weight).sum()
        return loss


class BboxLoss(nn.Module):

    def __init__(self, reg_max, use_dfl=False):
        """Initialize the BboxLoss module with regularization maximum and DFL settings."""
        super().__init__()
        self.reg_max = reg_max
        self.use_dfl = use_dfl

    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):
        """IoU loss."""

        
        weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)
        iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)
        loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum

        # DFL loss
        if self.use_dfl:
            target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)

            loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight
            loss_dfl = loss_dfl.sum() / target_scores_sum
        else:
            loss_dfl = torch.tensor(0.0).to(pred_dist.device)

        return loss_iou, loss_dfl

    @staticmethod
    def _df_loss(pred_dist, target):
        """Return sum of left and right DFL losses."""
        # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391
        tl = target.long()  # target left
        tr = tl + 1  # target right
        wl = tr - target  # weight left
        wr = 1 - wl  # weight right
        return (F.cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape) * wl +
                F.cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)


class KeypointLoss(nn.Module):

    def __init__(self, sigmas) -> None:
        super().__init__()
        self.sigmas = sigmas

    def forward(self, pred_kpts, gt_kpts, kpt_mask, area):
        """Calculates keypoint loss factor and Euclidean distance loss for predicted and actual keypoints."""
        d = (pred_kpts[..., 0] - gt_kpts[..., 0]) ** 2 + (pred_kpts[..., 1] - gt_kpts[..., 1]) ** 2
        kpt_loss_factor = (torch.sum(kpt_mask != 0) + torch.sum(kpt_mask == 0)) / (torch.sum(kpt_mask != 0) + 1e-9)
        # e = d / (2 * (area * self.sigmas) ** 2 + 1e-9)  # from formula
        e = d / (2 * self.sigmas) ** 2 / (area + 1e-9) / 2  # from cocoeval
        return kpt_loss_factor * ((1 - torch.exp(-e)) * kpt_mask).mean()
